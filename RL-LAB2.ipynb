{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Universidad del Valle de Guatemala  \n",
        "Aprendizaje por Refuerzo\n",
        "Alberto Suriano  \n",
        "\n",
        "Laboratorio 2  \n",
        "Marlon Hernández - 15177  \n",
        "\n",
        "- Link del repositorio: https://github.com/ivanhez/RL-LAB2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
        "1. **¿Qué es un Markov Decision Process (MDP)?**  \n",
        "    Un Proceso de Decisión de Markov (MDP) es un proceso de control estocástico en tiempo discreto. Proporciona un marco matemático para modelar la toma de decisiones en situaciones en las que los resultados son en parte aleatorios y en parte bajo el control de un decisor. \n",
        "\n",
        "2. **¿Cuáles son los componentes principales de un MDP?**  \n",
        "    Los MDP están compuestos por un conjunto de estados S, un conjunto de acciones A, una función de transición de estados T(s,a,s’) y una función de recompensa R(s,a,s’). La función de transición de estado determina la probabilidad de pasar al estado s’ dado que el estado actual es s y la acción realizada es a. La función de recompensa da la recompensa recibida al pasar del estado s al s’ dado que la acción realizada es a.\n",
        "    \n",
        "3. **¿Cuál es el objetivo principal del aprendizaje por refuerzo con MDPs?**  \n",
        "    El objetivo principal del aprendizaje por refuerzo con MDPs es encontrar una política óptima que maximice la recompensa acumulada a largo plazo para el agente. Una política (π) es una estrategia que define qué acción debe tomar el agente en cada estado. La política óptima π* maximiza el valor esperado de las recompensas acumuladas a lo largo del tiempo, considerando las transiciones de estado y las recompensas inmediatas. Esto implica que el agente debe aprender a tomar decisiones que no solo maximicen las recompensas inmediatas, sino que también consideren las consecuencias futuras de esas decisiones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2\n",
        "El objetivo principal de este ejercicio es que simule un MDP que represente un robot que navega por un laberinto de cuadrículas de 3x3 y evalúe una política determinada. Por ello considere, a un robot navega por un laberinto de cuadrícula de 3x3. El robot puede moverse en cuatro direcciones: arriba, abajo, izquierda y derecha. El objetivo es navegar desde la posición inicial hasta la posición de meta evitando obstáculos. El robot recibe una recompensa cuando alcanza la meta y una penalización si choca con un obstáculo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estados S: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
            "Acciones: ['arriba', 'abajo', 'izquierda', 'derecha']\n",
            "Start: 0\n",
            "Goal: 2\n",
            "Obstacles: {8, 4}\n",
            "+---+---+---+\n",
            "| 0 | 1 | 2 |\n",
            "+---+---+---+\n",
            "| 3 | 4 | 5 |\n",
            "+---+---+---+\n",
            "| 6 | 7 | 8 |\n",
            "+---+---+---+\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Estados\n",
        "S = list(range(9))\n",
        "print(f\"Estados S: {S}\")\n",
        "\n",
        "# Acciones\n",
        "A = [\"arriba\", \"abajo\", \"izquierda\", \"derecha\"]\n",
        "print(f\"Acciones: {A}\")\n",
        "\n",
        "start = 0\n",
        "goal = 2\n",
        "obstacles = {4, 8}\n",
        "\n",
        "laberinto = [\n",
        "    ['0', '1', '2'],\n",
        "    ['3', '4', '5'],\n",
        "    ['6', '7', '8']\n",
        "]\n",
        "print(f\"Start: {start}\")\n",
        "print(f\"Goal: {goal}\")\n",
        "print(f\"Obstacles: {obstacles}\")\n",
        "print(\"+---+---+---+\")\n",
        "for fila in laberinto:\n",
        "    print(\"|\", end=\"\")\n",
        "    for celda in fila:\n",
        "        print(f\" {celda} |\", end=\"\")\n",
        "    print(\"\\n+---+---+---+\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de transicion\n",
            "0: {'arriba': [(1.0, 0)], 'abajo': [(1.0, 3)], 'izquierda': [(1.0, 0)], 'derecha': [(1.0, 1)]}\n",
            "1: {'arriba': [(1.0, 1)], 'abajo': [(1.0, 1)], 'izquierda': [(1.0, 0)], 'derecha': [(1.0, 2)]}\n",
            "2: {'arriba': [(1.0, 2)], 'abajo': [(1.0, 5)], 'izquierda': [(1.0, 1)], 'derecha': [(1.0, 2)]}\n",
            "3: {'arriba': [(1.0, 0)], 'abajo': [(1.0, 6)], 'izquierda': [(1.0, 3)], 'derecha': [(1.0, 3)]}\n",
            "4: {'arriba': [], 'abajo': [], 'izquierda': [], 'derecha': []}\n",
            "5: {'arriba': [(1.0, 2)], 'abajo': [(1.0, 5)], 'izquierda': [(1.0, 5)], 'derecha': [(1.0, 5)]}\n",
            "6: {'arriba': [(1.0, 3)], 'abajo': [(1.0, 6)], 'izquierda': [(1.0, 6)], 'derecha': [(1.0, 7)]}\n",
            "7: {'arriba': [(1.0, 7)], 'abajo': [(1.0, 7)], 'izquierda': [(1.0, 6)], 'derecha': [(1.0, 7)]}\n",
            "8: {'arriba': [], 'abajo': [], 'izquierda': [], 'derecha': []}\n",
            "Funcion de recompensa\n",
            "0: {'arriba': {0: -5}, 'abajo': {3: 1}, 'izquierda': {0: -5}, 'derecha': {1: 1}}\n",
            "1: {'arriba': {1: -5}, 'abajo': {1: -10}, 'izquierda': {0: 1}, 'derecha': {2: 10}}\n",
            "2: {'arriba': {2: -5}, 'abajo': {5: 1}, 'izquierda': {1: 1}, 'derecha': {2: -5}}\n",
            "3: {'arriba': {0: 1}, 'abajo': {6: 1}, 'izquierda': {3: -5}, 'derecha': {3: -10}}\n",
            "4: {'arriba': {}, 'abajo': {}, 'izquierda': {}, 'derecha': {}}\n",
            "5: {'arriba': {2: 10}, 'abajo': {5: -10}, 'izquierda': {5: -10}, 'derecha': {5: -5}}\n",
            "6: {'arriba': {3: 1}, 'abajo': {6: -5}, 'izquierda': {6: -5}, 'derecha': {7: 1}}\n",
            "7: {'arriba': {7: -10}, 'abajo': {7: -5}, 'izquierda': {6: 1}, 'derecha': {7: -10}}\n",
            "8: {'arriba': {}, 'abajo': {}, 'izquierda': {}, 'derecha': {}}\n"
          ]
        }
      ],
      "source": [
        "# Matriz de transicion\n",
        "P = {s: {a: [] for a in A} for s in S}\n",
        "\n",
        "# Funcion de recompensa\n",
        "R = {s: {a: {} for a in A} for s in S}\n",
        "\n",
        "# Determinar el siguiente estado y la recompensa\n",
        "def get_next_state_reward(s, a):\n",
        "    row, col = divmod(s, 3)\n",
        "    if a == \"arriba\":\n",
        "        next_state = (row - 1, col) if row > 0 else (row, col)\n",
        "    elif a == \"abajo\":\n",
        "        next_state = (row + 1, col) if row < 2 else (row, col)\n",
        "    elif a == \"izquierda\":\n",
        "        next_state = (row, col - 1) if col > 0 else (row, col)\n",
        "    elif a == \"derecha\":\n",
        "        next_state = (row, col + 1) if col < 2 else (row, col)\n",
        "\n",
        "\n",
        "    next_state = next_state[0] * 3 + next_state[1]\n",
        " \n",
        "    if next_state == s:\n",
        "        reward = -5\n",
        "    elif next_state == goal:\n",
        "        reward = 10\n",
        "    elif next_state in obstacles:\n",
        "        reward = -10\n",
        "        next_state = s\n",
        "    else:\n",
        "        reward = 1\n",
        "\n",
        "    return next_state, reward\n",
        "\n",
        "for s in S:\n",
        "    if s not in obstacles:\n",
        "        for a in A:\n",
        "            next_state, reward = get_next_state_reward(s, a)\n",
        "            P[s][a].append((1.0, next_state))\n",
        "            R[s][a][next_state] = reward\n",
        "\n",
        "\n",
        "print(\"Matriz de transicion\")\n",
        "for key, value in P.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"Funcion de recompensa\")\n",
        "for key, value in R.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La política óptima es:\n",
            "derecha derecha abajo arriba arriba abajo arriba\n",
            "La recompensa promedio acumulada es: 11.0\n"
          ]
        }
      ],
      "source": [
        "# Generar una politica\n",
        "def generate_policy():\n",
        "    policy = {}\n",
        "    for s in S:\n",
        "        if s not in obstacles:\n",
        "            policy[s] = np.random.choice(A)\n",
        "    return policy\n",
        "\n",
        "# Simular la politica\n",
        "def simulate_policy(policy, start_state, steps):\n",
        "    state = start_state\n",
        "    total_reward = 0\n",
        "    for _ in range(steps):\n",
        "        action = policy[state]\n",
        "        probs, next_states = zip(*P[state][action])\n",
        "        next_state = np.random.choice(next_states, p=probs)\n",
        "        reward = R[state][action][next_state]\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "        if state == goal:\n",
        "            break\n",
        "    return total_reward\n",
        "\n",
        "# Evaluar una politica\n",
        "def evaluate_policy(policy, start_state, steps, simulations):\n",
        "    total_rewards = []\n",
        "    for i in range(simulations):\n",
        "        total_reward = simulate_policy(policy, start_state, steps)\n",
        "        total_rewards.append(total_reward)\n",
        "        # print(f\"Iteracion {i} Recompensa {total_reward}\")\n",
        "    average_reward = np.mean(total_rewards)\n",
        "    return average_reward\n",
        "\n",
        "# Encontrar la mejor politica\n",
        "def find_best_policy(start_state, steps, simulations, policy_trials):\n",
        "    best_policy = None\n",
        "    best_reward = -np.inf\n",
        "    for _ in range(policy_trials):\n",
        "        policy = generate_policy()\n",
        "        avg_reward = evaluate_policy(policy, start_state, steps, simulations)\n",
        "        if avg_reward > best_reward:\n",
        "            best_reward = avg_reward\n",
        "            best_policy = policy\n",
        "    return best_policy, best_reward\n",
        "\n",
        "policy_trials = 100\n",
        "simulations = 1000\n",
        "steps = 10\n",
        "\n",
        "best_policy, best_reward = find_best_policy(start, steps, simulations, policy_trials)\n",
        "\n",
        "print(\"La política óptima es:\")\n",
        "print(*list(best_policy.values()))\n",
        "print(f\"La recompensa promedio acumulada es: {best_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3\n",
        "En clase hemos dicho que una vez tengamos v* o q* sabemos la póliza óptima π* ¿Por qué?\n",
        "Puede consultar el libro en la sección 3.8 en adelante"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El tener v* o q* nos da el estado y el valor óptimo, por lo que  podemos determinar la mejor acción a tomar en cada estado para maximizar la recompensa acumulada a largo plazo, siendo esto la política óptima π*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Referencias\n",
        "* https://python.plainenglish.io/understanding-markov-decision-processes-17e852cd9981\n",
        "* https://medium.com/@ngao7/markov-decision-process-basics-3da5144d3348\n",
        "* https://techlib.net/techedu/proceso-de-decision-de-markov-mdp/\n",
        "* https://medium.com/@TechInsight/reinforcement-learning-part-3-d037fa27c5cb"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
